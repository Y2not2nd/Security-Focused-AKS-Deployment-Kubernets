Production-Ready Secure AKS Deployment
 (Terraform, Kubernetes & GitOps)
 Overview
 This guide provides a complete production-ready example of a secure Azure Kubernetes Service (AKS)
 project. It covers everything from provisioning a private AKS cluster with Terraform to deploying a full
 application stack with Helm charts and managing it via Argo CD (GitOps). Key components include: 
• 
• 
• 
• 
• 
• 
• 
• 
Infrastructure: AKS cluster (UK South) with private API endpoint, RBAC enabled, network
 policies via Calico/Azure, and minimal IAM (managed identities). An Azure Container Registry
 (ACR) is used for images.
 Frontend: An Angular web application served by Nginx (packaged as a Helm chart).
 Backend: A Go REST API connecting to MongoDB, with secrets provided by HashiCorp Vault
 (Kubernetes auth and Vault Agent Injector for secret injection).
 1
 2
 Service Mesh: Istio service mesh enforcing strict mTLS for all pod-to-pod traffic, plus an Istio
 IngressGateway, VirtualService for routing, and AuthorizationPolicies to restrict internal access.
 Secrets Management: HashiCorp Vault is deployed in Dev mode (in-cluster) to manage sensitive
 data (like database credentials), using the Kubernetes Auth method to allow the backend to
 retrieve secrets securely .
 Monitoring: A Prometheus and Grafana stack (via the kube-prometheus-stack Helm chart)
 monitors cluster and app metrics. Pre-configured dashboards cover cluster health, Istio service
 mesh metrics, MongoDB, and custom app metrics.
 GitOps (Argo CD): Argo CD is used with an App-of-Apps pattern – a root Argo CD Application
 bootstraps the deployment of all other applications from a single Git repository. This ensures the
 entire stack (infrastructure as code, application manifests, and policies) is under version control
 and deployable with one sync operation.
 Security Scanning: Snyk is integrated into the workflow to scan Terraform code and container
 images for vulnerabilities and misconfigurations (infrastructure code is checked for RBAC
 enforcement, etc.
 3
 4
 , and images are scanned for known CVEs before deployment).
 All code is organized in a Git repository structure. Below is the project folder layout (as it would appear
 in a Git repo):
 aks-secure-deployment/
 ├── infrastructure/
 │   ├── main.tf
 │   ├── variables.tf
 │   ├── outputs.tf
 │   └── provider.tf
 ├── src/
 │   ├── backend/
 │   │   ├── main.go
 │   │   ├── go.mod
 │   │   └── Dockerfile
 1
│   └── frontend/
 │       ├── src/
 │       │   ├── app/
 │       │   │   ├── app.module.ts
 │       │   │   ├── app.component.ts
 │       │   │   ├── app.component.html
 │       │   │   └── api.service.ts
 │       │   ├── index.html
 │       │   ├── main.ts
 │       │   └── polyfills.ts
 │       ├── environments/
 │       │   └── environment.ts
 │       ├── package.json
 │       ├── angular.json
 │       ├── tsconfig.json
 │       └── Dockerfile
 ├── helm-charts/
 │   ├── frontend/
 │   │   ├── Chart.yaml
 │   │   ├── values.yaml
 │   │   └── templates/
 │   │       ├── deployment.yaml
 │   │       ├── service.yaml
 │   │       └── configmap.yaml
 │   └── backend/
 │       ├── Chart.yaml
 │       ├── values.yaml
 │       └── templates/
 │           ├── deployment.yaml
 │           ├── service.yaml
 │           ├── hpa.yaml
 │           └── networkpolicy.yaml
 ├── argocd-apps/
 │   ├── root-app.yaml
 │   ├── vault-app.yaml
 │   ├── istio-base-app.yaml
 │   ├── istio-cp-app.yaml
 │   ├── istio-gateway-app.yaml
 │   ├── istio-config-app.yaml
 │   ├── backend-app.yaml
 │   ├── frontend-app.yaml
 │   ├── mongodb-app.yaml
 │   └── monitoring-app.yaml
 ├── istio-config/
 │   ├── peer-auth.yaml
 │   ├── gateway.yaml
 │   ├── virtual-service.yaml
 │   └── auth-policy.yaml
 └── scripts/
    └── teardown.sh
 2
Each section below describes these components in detail, with full file contents and configuration. By
 following this guide and using the provided files, you can directly deploy the entire stack (apply
 Terraform, build and push images, then use Argo CD to sync all manifests). Short demo instructions are
 included to verify that the frontend, backend, and data layers are working together securely. 
Infrastructure Provisioning with Terraform
 First, we provision the AKS cluster and supporting resources using Terraform. The Terraform code
 creates: 
• 
• 
• 
• 
• 
• 
• 
• 
A new Resource Group and Virtual Network (with a dedicated subnet for AKS).
 An AKS cluster with:
 Private API endpoint (
 1
 private_cluster_enabled = true ), so the Kubernetes API is
 accessible only within the Azure virtual network (enhancing security ).
 RBAC enabled (Role-Based Access Control) for Kubernetes (Azure AD integration for cluster
 authentication, with no static admin credentials) .
 Network policies enabled (using Azure CNI with Calico/Azure policies to restrict pod traffic at
 the network level).
 4
 Managed identities instead of service principals for minimal IAM footprint (the cluster’s identity
 is used to access Azure resources like ACR).
 Local accounts disabled (
 local_account_disabled = true ) to remove the default
 Kubernetes admin user (mitigates the risk of leaked admin kubeconfig credentials
 2
 – cluster
 access is via Azure AD only).
 An Azure Container Registry (ACR) for container images, and a role assignment that grants the
 AKS nodepool’s identity pull access on the ACR (so that the cluster can pull private images). 
Below are the Terraform files in the 
infrastructure/ directory:
 infrastructure/provider.tf – Terraform settings and Azure provider configuration:
 terraform {
 required_providers {
 azurerm = {
 source = "hashicorp/azurerm"
 version = "~> 3.70"
    # Use a recent version of the Azure provider
 }
 }
 }
 provider "azurerm" {
 features {}
 needed)
 }
               # Enable all AZ resources (no special features 
infrastructure/variables.tf – Input variables for reusability:
 variable "prefix" {
 description =
 "Prefix for naming Azure resources (should be unique to avoid name 
collisions)"
 3
type
 default
 = string
 = "akssecuredemo"
 }
 variable "location" {
 description = "Azure region for all resources"
 type
 default
 = string
 = "UK South"
 }
 variable "admin_group_object_id" {
 description = "Azure AD group Object ID to grant AKS Cluster Admin role 
(for Kubernetes RBAC via AAD)"
 type
 }
 = string
 Explanation: The 
prefix will be used to name resources (like the resource group, AKS cluster, etc.).
 Note: You should set 
admin_group_object_id to the Azure AD group (or user) that should have
 admin access to the AKS cluster. This enables Azure AD integration for AKS – only identities in that AAD
 group will have cluster-admin permissions. If not set, Terraform will error because
 local_account_disabled is true (no admin kubeconfig will be issued). Ensure you have an AAD
 group ready and supply its GUID.
 infrastructure/main.tf – Main Terraform configuration to create the network, cluster, and
 registry:
 resource "azurerm_resource_group" "rg" {
 name
 = "${var.prefix}-rg"
 location = var.location
 }
 resource "azurerm_virtual_network" "vnet" {
 name
 = "${var.prefix}-vnet"
 address_space
 location
 = ["10.50.0.0/16"]
 = var.location
 resource_group_name = azurerm_resource_group.rg.name
 }
 resource "azurerm_subnet" "aks" {
 name
 = "${var.prefix}-aks-subnet"
 resource_group_name = azurerm_resource_group.rg.name
 virtual_network_name = azurerm_virtual_network.vnet.name
 address_prefixes
 = ["10.50.0.0/22"]  # Subnet for AKS nodes (and pods, 
since using Azure CNI)
 }
 resource "azurerm_kubernetes_cluster" "aks" {
 name
 = "${var.prefix}-aks"
 location
 = var.location
 resource_group_name = azurerm_resource_group.rg.name
 kubernetes_version = "1.24.9"  # or latest available stable version
 4
dns_prefix = "${var.prefix}-aks"   # used for AKS DNS name; not 
exposed publicly in private cluster
 api_server_authorized_ip_ranges = []        # no public API access (empty 
list means all public IPs blocked)
 default_node_pool {
 name = "system"
 node_count = 3
 vm_size = "Standard_D2s_v3"
 vnet_subnet_id = azurerm_subnet.aks.id
 type = "VirtualMachineScaleSets"
 node_labels = {
 "agentpool" = "system"
 }
 }
 identity {
 type = "SystemAssigned"  # use a managed identity for the AKS control 
plane
 }
 network_profile {
 network_plugin = "azure"   # Azure CNI (pods get VNet IPs)
 network_policy = "azure"   # enable Azure network policies (or 
"calico")
 load_balancer_sku = "standard"
 service_cidr = "10.0.0.0/16"       # Cluster IPs for services
 dns_service_ip = "10.0.0.10"         # DNS service IP within service 
CIDR
 docker_bridge_cidr = "172.17.0.1/16"
 outbound_type = "loadBalancer"      # Outbound traffic uses standard 
LB
 }
 private_cluster_enabled =
 true            # Enable private API endpoint (no public IP for AKS API)
 private_dns_zone_id = null            # Use AKS-managed private DNS 
zone for API server
 local_account_disabled = true            # Disable Kubernetes admin user 
(Azure AD only)
 role_based_access_control {
 enabled = true
 azure_active_directory {
 managed = true
 admin_group_object_ids = [var.admin_group_object_id]
 }
 }
 tags = {
 Environment = "Demo"
 5
}
 }
 resource "azurerm_container_registry" "acr" {
 name
 location
 = "${var.prefix}acr"
 = var.location
 resource_group_name = azurerm_resource_group.rg.name
 sku
 = "Standard"
 admin_enabled
 }
 = false
    # disable admin user, use AAD integration
 # Grant AKS permission to pull from ACR (AcrPull role)
 resource "azurerm_role_assignment" "acr_pull" {
 scope
 = azurerm_container_registry.acr.id
 role_definition_name = "AcrPull"
 principal_id
 =
 azurerm_kubernetes_cluster.aks.identity[0].principal_id
 }
 In the above, the AKS cluster is configured to be as secure as possible out-of-the-box. It has no public
 IP for the API server (
 private_cluster_enabled 
= 
true ), and by leaving
 api_server_authorized_ip_ranges empty we ensure no IP can access the API except through the
 private endpoint. Kubernetes RBAC is enabled with Azure AD – only identities in the provided AAD group
 will be cluster admins (regular users can be granted access via Azure RBAC for Kubernetes or in-cluster
 RoleBindings as needed). The cluster’s managed identity will be used to automatically create Azure
 resources like load balancers and to authenticate to ACR. We disable the legacy admin account to
 enforce use of Azure AD credentials only. Networking is locked down: we chose the Azure CNI plugin so
 pods get first-class IPs in the VNet, and enabled Azure network policies for an extra layer of isolation
 (you could also use Calico policies; Azure supports both ). The node subnet (
 5
 10.50.0.0/22 ) is
 sized for a modest cluster (around 3–4 nodes with room for pod IPs); adjust this CIDR larger for more
 nodes. 
After the cluster is created, we deploy an ACR. The 
azurerm_role_assignment binds the AKS
 cluster’s identity to the AcrPull role on the registry, allowing the cluster’s nodes to pull images from ACR
 securely (without needing ACR’s admin password). 
infrastructure/outputs.tf – Output values from Terraform:
 output "aks_name" {
 description = "AKS cluster name"
 value
 = azurerm_kubernetes_cluster.aks.name
 }
 output "acr_login_server" {
 description = "ACR login server (for image pushes)"
 value
 }
 = azurerm_container_registry.acr.login_server
 6
After running Terraform, these outputs will help in the next steps. The 
acr_login_server output
 (e.g. 
akssecuredemoacr.azurecr.io ) will be used to tag and push our application images. The
 aks_name is mostly informational (it will be prefixed with our 
prefix variable).
 Applying Terraform
 To provision the infrastructure, run the following commands (ensure you’re authenticated to Azure via
 az login and have permission to create these resources): 
# Navigate into the infrastructure directory
 cd infrastructure/
 # Initialize Terraform and Azure provider plugins
 terraform init
 # Review the plan (optional)
 terraform plan-var="admin_group_object_id=<YOUR_AAD_GROUP_ID>"
 # Apply the plan to create resources
 terraform apply-var="admin_group_object_id=<YOUR_AAD_GROUP_ID>"-auto
approve
 Provide your Azure AD admin group Object ID as the 
admin_group_object_id variable. Once this
 f
 inishes, you will have a resource group, VNet, ACR, and AKS cluster deployed. Important: Because this
 is a private AKS cluster, you need network access to the cluster to use 
kubectl . Typically, you would
 use an Azure VM on the same VNet (or Azure Bastion/ExpressRoute/VPN). For simplicity, you can use
 Azure Cloud Shell or a jumpbox VM to run the 
kubectl and Argo CD commands that follow, since
 Cloud Shell will be able to connect to the private cluster endpoint. 
Retrieve the Kubernetes credentials (if using Azure CLI on a machine that has network access to the
 cluster or via Cloud Shell):
 az aks get-credentials-g "$(terraform output-raw aks_name | sed 's/-aks$/
rg/')"-n "$(terraform output-raw aks_name)"--admin
 This uses the Azure CLI to fetch cluster admin credentials. We use the --admin flag here to get a
 kubeconfig using the cluster’s admin context (since Azure AD RBAC is enabled, you’d normally get a
 token tied to your AAD user; using --admin in this case is acceptable because the API endpoint is not
 exposed publicly). If 
local_account_disabled=true as we set, Azure might not issue admin
 credentials at all – in that case, you would instead use an AAD user’s token. For demo purposes, you can
 temporarily set 
local_account_disabled = false if needed to fetch credentials, or use Azure AD
 authentication by ensuring your AAD user is in the admin group and running 
credentials -n <name> -g <rg> (no --admin ). 
az aks get
Verify: Ensure you can connect to the cluster. For example, check nodes and confirm network policy
 provider: 
7
kubectl get nodes
 kubectl get pods-A
 At this point, the cluster is running but empty. Next, we will build the application containers and set up
 the GitOps deployment.
 Application Components and Container Images
 The application is a simple web app with a Go backend and Angular frontend, demonstrating secure
 communication and data storage:
 • 
• 
• 
• 
The Go backend exposes a few API endpoints (in our case, just a 
/api/ping endpoint for
 demonstration) and connects to MongoDB. It uses the official MongoDB Go driver to increment
 a visit counter in a MongoDB collection. The MongoDB connection string (with credentials) is not
 baked into the image – instead, it’s provided at runtime via Vault Agent Injector (the backend
 reads it from a file that Vault writes). This keeps sensitive credentials out of the code and out of
 Kubernetes manifests.
 The Angular frontend is a single-page application that calls the backend API. It’s served by an
 Nginx web server. For simplicity, the frontend has a single page with a button that triggers an
 API call to the backend and displays the result. The Angular app calls the relative URL 
/api/
 ping , which will be routed (by Istio) to the backend service. 
MongoDB stores the data (a single document with a counter). We deploy MongoDB using
 Bitnami’s Helm chart (configured for a single instance). Authentication is enabled on Mongo; we
 create a specific user (
 appuser ) and password for the backend to use. These credentials are
 stored in Vault so the backend can retrieve them. 
All components run in Kubernetes. Istio is used to secure traffic: the Angular frontend (Nginx)
 and Go backend pods both have Istio sidecars, so calls between them (through the Istio ingress
 gateway) are encrypted with mTLS. We also define an Istio AuthorizationPolicy so that only the
 frontend is allowed to call the backend service (any other pod in the mesh would be denied by
 Istio, even if network policies allowed it). 
Let's go through each component’s implementation, including the Dockerfile and Kubernetes
 manifests/Helm charts.
 Go Backend API (Go + Vault Integration)
 Source Code (
 src/backend/main.go ): This is a Go application implementing a basic HTTP server. It
 connects to MongoDB and defines a 
/api/ping handler which increments a counter in the database
 and returns the result. It reads the MongoDB connection URI from a file (
 creds.txt ) that will be populated by Vault Agent. 
package main
 import (
 "context"
 "fmt"
 "log"
 "net/http"
 "os"
 /vault/secrets/db
8
"time"
 "encoding/json"
 "go.mongodb.org/mongo-driver/bson"
 "go.mongodb.org/mongo-driver/mongo"
 "go.mongodb.org/mongo-driver/mongo/options"
 )
 var collection *mongo.Collection
 type Counter struct {
 ID string `bson:"_id"`
 Count int `bson:"count"`
 }
 func main() {
 // Read MongoDB connection string from Vault-injected file
 uriBytes, err := os.ReadFile("/vault/secrets/db-creds.txt")
 if err != nil {
 log.Fatalf("Failed to read Mongo URI from file: %v", err)
 }
 uri := string(uriBytes)
 // Connect to MongoDB
 ctx, cancel := context.WithTimeout(context.Background(), 10*time.Second)
 defer cancel()
 client, err := mongo.Connect(ctx, options.Client().ApplyURI(uri))
 if err != nil {
 log.Fatalf("Mongo connect error: %v", err)
 }
 if err = client.Ping(ctx, nil); err != nil {
 log.Fatalf("Mongo ping error: %v", err)
 }
 db := client.Database("mydb") // Database name as per 
connection URI
 collection =
 db.Collection("visits") // Collection to store visit count
 http.HandleFunc("/api/ping", pingHandler)
 log.Println("Backend server is running on port 8080...")
 log.Fatal(http.ListenAndServe(":8080", nil))
 }
 func pingHandler(w http.ResponseWriter, r *http.Request) {
 if r.Method != http.MethodGet {
 http.Error(w, "Invalid method", http.StatusMethodNotAllowed)
 return
 }
 ctx, cancel := context.WithTimeout(context.Background(), 5*time.Second)
 defer cancel()
 9
// Atomically increment the counter document in MongoDB
 opts :=
 options.FindOneAndUpdate().SetUpsert(true).SetReturnDocument(options.After)
 result := collection.FindOneAndUpdate(ctx,
 bson.M{"_id": "counter"},
 bson.M{"$inc": bson.M{"count": 1}},
 opts,
 )
 if result.Err() != nil {
 log.Printf("DB update error: %v", result.Err())
 http.Error(w, "Database error", http.StatusInternalServerError)
 return
 }
 var counter Counter
 if err := result.Decode(&counter); err != nil {
 log.Printf("DB decode error: %v", err)
 }
 msg := fmt.Sprintf("Hello, you are visitor #%d", counter.Count)
 w.Header().Set("Content-Type", "application/json")
 json.NewEncoder(w).Encode(map[string]string{"message": msg})
 }
 A few notes on the backend: - It expects the MongoDB connection string (including credentials and
 database name) to be available at 
/vault/secrets/db-creds.txt . We will configure Vault to
 provide this file to the pod (via the Vault Agent Injector). The code reads the file at startup and uses the
 URI to connect to MongoDB. - We use a FindOneAndUpdate with 
"$inc" to atomically increment a
 counter document (with 
_id: "counter" ). If it doesn’t exist, 
upsert=true creates it. This ensures
 even if two requests hit at nearly the same time, the counter increments without conflict. - The HTTP
 handler returns a JSON response like 
{"message": "Hello, you are visitor #3"} . This is what
 the Angular frontend will display. - Logging is used for any errors (e.g., if the DB isn’t reachable or Vault
 f
 ile not present). In a real app, you might have more robust error handling and health checks.
 Go Module (
 src/backend/go.mod ): This defines the module name and dependencies, including the
 MongoDB driver: 
module akssecure/backend
 go 1.19
 require go.mongodb.org/mongo-driver v1.17.3
 We use Go 1.19 here and pin the Mongo driver to version 1.17.3 (a stable release). Go modules will also
 generate a 
go.sum when dependencies are downloaded (not shown here for brevity). 
Dockerfile for Backend (
 src/backend/Dockerfile ): We build a minimal container for the Go app.
 The build uses a multi-stage Docker build to produce a small, secure image: 
10
# Stage 1: Build the Go binary
 FROM golang:1.19-alpine AS builder
 WORKDIR /app
 COPY go.mod ./
 COPY go.sum ./
 RUN go mod download
 COPY . .
 RUN CGO_ENABLED=0 go build-o backend .
 # Stage 2: Create a lightweight image for running
 FROM gcr.io/distroless/base-debian11
 COPY--from=builder /app/backend /backend
 # Run as non-root user for security
 USER nonroot:nonroot
 EXPOSE 8080
 ENTRYPOINT ["/backend"]
 Security best practices in the Dockerfile: We use the Distroless base image for the final image (which
 has no shell and includes only the minimal libraries needed). We also specify a non-root user
 (nonroot) to run the application. The Go binary is statically linked (
 CGO_ENABLED=0 ) so it doesn’t
 depend on OS libraries. The resulting image is very small and has a minimal attack surface (no package
 manager, shell, or extra utilities). Only port 8080 is exposed. 
Angular Frontend (Angular + Nginx)
 The Angular frontend is a simple application that will call the backend API. We use Angular 16 (you can
 use any recent version) and serve the compiled app with Nginx. We’ll build the Angular code into static
 f
 iles and then use an Nginx container to serve it. The Nginx config is set up to route all requests to
 index.html (for SPA routing) except API calls, which are forwarded to the backend via Istio.
 Angular App Module (
 src/frontend/src/app/app.module.ts ): Basic Angular module that
 imports HttpClient: 
import { BrowserModule } from '@angular/platform-browser';
 import { NgModule } from '@angular/core';
 import { HttpClientModule } from '@angular/common/http';
 import { AppComponent } from './app.component';
 import { ApiService } from './api.service';
 @NgModule({
 declarations: [ AppComponent ],
 imports: [ BrowserModule, HttpClientModule ],
 providers: [], // ApiService is providedIn: 'root'
 bootstrap: [ AppComponent ]
 })
 export class AppModule { }
 11
Angular Component (
 src/frontend/src/app/app.component.ts ): A simple component with a
 button to ping the backend: 
import { Component } from '@angular/core';
 import { ApiService } from './api.service';
 @Component({
 selector: 'app-root',
 templateUrl: './app.component.html'
 })
 export class AppComponent {
 title = 'Secure AKS Demo';
 backendMessage: string = '';
 constructor(private api: ApiService) {}
 pingBackend(): void {
 this.api.ping().subscribe(
 data => {
 this.backendMessage = data.message;
 },
 error => {
 console.error('API error:', error);
 this.backendMessage = 'Error: ' + error.statusText;
 }
 );
 }
 }
 This component has a 
<h1>{{ title }}</h1>
 <button (click
 pingBackend() method that calls our ApiService and stores the response
 message. It also handles errors by logging to console and showing a basic error text.
 Component Template (
 src/frontend/src/app/app.component.html ):
 )="pingBackend
 ()">Ping Backend</button>
 <div *ngIf="backendMessage">{{ backendMessage }}</div>
 The UI is very minimal: a title, a button, and a div to display the message from the backend. Initially 
backendMessage is empty, and after clicking the button it will show something like "Hello, you are
 visitor #X" if successful.
 API Service (
 src/frontend/src/app/api.service.ts ): Angular service to call the backend: 
import { Injectable } from '@angular/core';
 import { HttpClient } from '@angular/common/http';
 import { Observable } from 'rxjs';
 12
@Injectable({ providedIn: 'root' })
 export class ApiService {
 constructor(private http: HttpClient) {}
 ping(): Observable<any> {
 // Calls the backend API (relative URL will be proxied via Istio to the 
backend service)
 return this.http.get<any>('/api/ping');
 }
 }
 Because we call 
/api/ping on the same origin, no additional CORS configuration is needed – the
 request will go to the Istio IngressGateway, which will route it to the backend service. The Angular app
 doesn’t need to know the full URL or IP of the backend, it just uses the relative path.
 Main entry point (
 src/frontend/src/main.ts ): Standard Angular bootstrap: 
import { platformBrowserDynamic } from '@angular/platform-browser-dynamic';
 import { AppModule } from './app/app.module';
 platformBrowserDynamic().bootstrapModule(AppModule)
 .catch(err => console.error(err));
 Polyfills (
 src/frontend/src/polyfills.ts ): We import Zone.js (required by Angular): 
import 'zone.js'; // Included with Angular CLI.
 Index HTML (
 src/frontend/src/index.html ): Basic HTML template: 
<!doctype html>
 <html lang="en">
 <head>
 <meta charset="utf-8">
 <title>Secure AKS Demo</title>
 <base href="/" />
 </head>
 <body>
 <app-root></app-root>
 </body>
 </html>
 Angular Environment (
 src/frontend/environments/environment.ts ):
 13
export const environment = {
 production: false
 };
 (In this simple app we don’t use environment variables for the API URL – we call the relative path
 directly. In a more complex setup, you might configure 
environment.apiUrl .)
 Angular Configuration (
 src/frontend/angular.json ): Key configuration for building the Angular
 app: 
{
 }
 "$schema": "./node_modules/@angular/cli/lib/config/schema.json",
 "version": 1,
 "projects": {
 "frontend": {
 "projectType": "application",
 "sourceRoot": "src",
 "architect": {
 "build": {
 "options": {
 "outputPath": "dist/frontend",
 "index": "src/index.html",
 "main": "src/main.ts",
 "polyfills": "src/polyfills.ts"
 }
 }
 }
 }
 }
 This defines the output path for the build (
 dist/frontend ), which we will use in the Dockerfile. We
 have omitted some sections for brevity (like configurations for dev/prod) since the defaults will work for
 this demo.
 Package.json (
 src/frontend/package.json ): Key scripts and dependencies: 
{
 "name": "frontend",
 "version": "0.0.1",
 "scripts": {
 "start": "ng serve",
 "build": "ng build --prod"
 },
 "dependencies": {
 "@angular/common": "~16.0.0",
 "@angular/core": "~16.0.0",
 "@angular/platform-browser": "~16.0.0",
 14
"@angular/platform-browser-dynamic": "~16.0.0",
 "@angular/router": "~16.0.0",
 "rxjs": "~7.5.0",
 "tslib": "^2.3.0",
 "zone.js": "~0.12.0"
 },
 "devDependencies": {
 "@angular/cli": "~16.0.0",
 "@angular/compiler-cli": "~16.0.0",
 "typescript": "~4.8.4"
 }
 }
 This is a trimmed-down package.json focusing on the Angular dependencies. The important part is that
 it has Angular and RxJS. The build script 
ng build --prod (which uses the Angular CLI) will produce
 an optimized production build in the 
dist/frontend folder.
 Dockerfile for Frontend (
 src/frontend/Dockerfile ): We build the Angular app and serve it via
 Nginx: 
# Stage 1: Build Angular app
 FROM node:18-alpine AS builder
 WORKDIR /app
 COPY package.json package-lock.json ./
 RUN npm ci
 # Install dependencies (clean install)
 COPY . .
 RUN npm run build--prod
 # Stage 2: Serve with Nginx
 FROM nginx:alpine
 COPY--from=builder /app/dist/frontend /usr/share/nginx/html
 # Copy custom Nginx config for SPA routing
 COPY nginx.conf /etc/nginx/conf.d/default.conf
 We use a multi-stage build: first stage uses Node.js 18 to compile the Angular app (the 
ensures a clean install of exact dependency versions from 
npm ci
 package-lock.json ). The result is static
 f
 iles in 
dist/frontend . The second stage uses the lightweight Nginx Alpine image. We copy the
 static files into Nginx’s default web root. We also provide a custom Nginx configuration
 (default.conf) to handle routing. Specifically, the config will ensure that requests for application
 paths are served the 
index.html (so that Angular’s client-side router can handle them), while
 requests to 
/api/* will be proxied to the backend service (if we were configuring Nginx to do so).
 However, since we will use Istio for routing, we actually do not need Nginx to proxy 
/api – the
 browser will call 
/api which goes through the Istio gateway. So, our Nginx config simply needs to
 serve the app and not interfere with 
/api calls (other than not trying to serve them as static files). 
Nginx Config (
 helm-charts/frontend/templates/configmap.yaml ): Instead of embedding
 nginx.conf in the Docker image, we supply it via a ConfigMap mounted into the Nginx container
 15
(this makes it easier to tweak without rebuilding the image). Our config ensures any path that isn’t a
 real file falls back to 
index.html : 
apiVersion: v1
 kind: ConfigMap
 metadata:
 name: frontend-nginx-config
 data:
 default.conf: |
 server {
 listen 80;
 server_name _;
 root /usr/share/nginx/html;
 index index.html;
 location / {
 try_files $uri $uri/ /index.html =404;
 }
 }
 This config listens on port 80 and serves files from 
/usr/share/nginx/html (where our Angular app
 is). The 
try_files directive means: - If the request matches an actual file (
 ($uri/), serve it. - Otherwise, respond with 
$uri ) or directory
 index.html . This lets the Angular app handle unknown
 paths (for example, if the app had client-side routes). - The 
=404 is just a fallback to send 404 if even
 index.html can’t be served (shouldn’t happen in our case).
 Because we are not doing any API proxying in Nginx (Istio will handle it), we don’t need additional
 proxy_pass rules here. All 
/api calls will not find a file (there’s no 
/api directory), so Nginx will serve
 index.html . The Angular app makes an XHR to 
/api/ping , but that request won’t actually go to
 Nginx for content; it will be intercepted by Istio’s sidecar and routed to the backend service. In practice,
 because our Istio Gateway will route 
^/api requests to the backend, the Nginx container just sees
 requests for 
/api/ping as incoming to port 80. However, since those are not static files, if Istio
 sidecar wasn’t there it would have served index.html. With Istio sidecar, the request is transparently
 handled by Envoy and forwarded to backend, so Nginx doesn’t actually process it. The config above is
 still good to have for any non-API deep links (ensuring the SPA works if the user refreshes the page on a
 nested route).
 Building and Pushing Images to ACR
 After coding the backend and frontend, we need to containerize them and push to our Azure Container
 Registry so they can be pulled by the AKS cluster. Assuming you have the Terraform outputs or know
 your ACR name, use the following steps:
 1. 
Log in to ACR: Use Azure CLI or Docker to authenticate. 
az acr login--name <your_ACR_name>
 (Replace 
<your_ACR_name> with the name of your registry, e.g., 
akssecuredemoacr .)
 16
Build and push the backend image:
 2. 
3. 
cd src/backend
 docker build-t <your_ACR_name>.azurecr.io/backend:latest .
 docker push <your_ACR_name>.azurecr.io/backend:latest
 This will compile the Go app and produce an image tagged with your ACR’s login server and
 repository name “backend”.
 Build and push the frontend image:
 cd ../frontend
 docker build-t <your_ACR_name>.azurecr.io/frontend:latest .
 docker push <your_ACR_name>.azurecr.io/frontend:latest
 The Angular build may take a minute or two. Once done, you push the Nginx-serving-frontend
 image to ACR.
 After these steps, both images (
 frontend:latest and 
backend:latest in your ACR) are available
 to the AKS cluster. We can now deploy everything on Kubernetes using Helm charts and Argo CD.
 Kubernetes Deployment Manifests (Helm Charts & YAML)
 Now we deploy all components onto the AKS cluster. We use Helm charts for the frontend and backend
 apps, and official Helm charts for Istio, Vault, and monitoring. All of these are orchestrated by Argo CD
 (coming up in the next section). For clarity, we’ll first describe the Kubernetes manifests (Helm
 templates or raw YAML) for each component:
 Frontend Deployment (Angular + Nginx on Kubernetes)
 We package the frontend as a Helm chart (
 helm-charts/frontend ). This chart defines a Deployment
 for the Angular+Nginx pod, a Service to expose it internally, and the ConfigMap for Nginx config. The
 image for the frontend will be the one we pushed to ACR.
 Chart definition (
 helm-charts/frontend/Chart.yaml ):
 apiVersion: v2
 name: frontend
 description: Helm chart for Angular frontend (Nginx)
 version: 0.1.0
 appVersion: "1.0"
 Default values (
 helm-charts/frontend/values.yaml ):
 17
replicaCount: 2
 image:
 repository: <ACR_NAME>.azurecr.io/frontend
 tag: "latest"
 pullPolicy: IfNotPresent
 service:
 type: ClusterIP
 port: 80
 Replace 
<ACR_NAME> with your actual ACR name (or supply it via values when installing with Argo CD).
 We set 2 replicas for the frontend by default (for high availability). The service is ClusterIP (we won’t
 expose it directly; Istio ingress will be used for external access).
 Deployment template (
 helm-charts/frontend/templates/deployment.yaml ):
 apiVersion: apps/v1
 kind: Deployment
 metadata:
 name: frontend
 labels:
 app: frontend
 spec:
 replicas: {{ .Values.replicaCount }}
 selector:
 matchLabels:
 app: frontend
 template:
 metadata:
 labels:
 app: frontend
 annotations:
 sidecar.istio.io/inject: "true"
 spec:
 containers:- name: frontend
 image: "{{ .Values.image.repository }}:{{ .Values.image.tag }}"
 imagePullPolicy: {{ .Values.image.pullPolicy }}
 ports:- containerPort: 80
 volumeMounts:- name: nginx-config
 mountPath: /etc/nginx/conf.d
 volumes:- name: nginx-config
 configMap:
 name: frontend-nginx-config
 18
Key points: - We label pods with 
app: frontend (so the Service and any network policies or Istio
 rules can target it). - We annotate the pod with 
sidecar.istio.io/inject: "true" . This forces
 Istio to inject an Envoy sidecar even if the namespace might not be labeled for injection. We want the
 frontend inside the mesh to enforce mTLS. - The Nginx config from our earlier ConfigMap is mounted at
 /etc/nginx/conf.d . This will override the default Nginx config with our custom 
default.conf . 
The container uses the image from ACR we built (tagged “latest”). It listens on port 80.
 Service template (
 helm-charts/frontend/templates/service.yaml ):
 apiVersion: v1
 kind: Service
 metadata:
 name: frontend
 labels:
 app: frontend
 spec:
 type: {{ .Values.service.type }}
 ports:- port: {{ .Values.service.port }}
 targetPort: 80
 protocol: TCP
 name: http
 selector:
 app: frontend
 This creates a Kubernetes service 
frontend on port 80. We’ll later route traffic to this service via Istio.
 It’s internal (ClusterIP), meaning it’s only accessible inside the cluster/Istio mesh. 
Nginx Config ConfigMap already shown above (
 frontend-nginx-config ). The chart includes it
 (as 
templates/configmap.yaml ). We ensure that it’s applied before the Deployment (so that the
 volume mount finds it). In Argo CD, this is usually handled by syncing everything together, but if
 needed, we could set hook ordering. It’s a simple config to enable the Angular app’s client-side routing.
 Note on resource limits and HPA: For brevity, the frontend chart does not set CPU/memory limits or
 an HorizontalPodAutoscaler. The Angular Nginx pods are static content servers and generally low on
 CPU. You can add 
resources: limits in values and a simple HPA if needed (similar to how it’s done for
 the backend below). The main autoscaling we demonstrate is for the backend.
 Backend Deployment (Go API + Vault Injector)
 The backend is also packaged as a Helm chart (
 helm-charts/backend ). This chart includes the
 Deployment, Service, an HPA (Horizontal Pod Autoscaler), and a NetworkPolicy for database access. 
Chart definition (
 helm-charts/backend/Chart.yaml ):
 apiVersion: v2
 name: backend
 description: Helm chart for Go backend API
 19
version: 0.1.0
 appVersion: "1.0"
 Default values (
 helm-charts/backend/values.yaml ):
 replicaCount: 2
 image:
 repository: <ACR_NAME>.azurecr.io/backend
 tag: "latest"
 pullPolicy: IfNotPresent
 service:
 type: ClusterIP
 port: 8080
 resources:
 limits:
 cpu: 500m
 memory: 256Mi
 requests:
 cpu: 200m
 memory: 128Mi
 hpa:
 enabled: true
 maxReplicas: 5
 cpuTargetPercentage: 50
 We configure the backend to autoscale based on CPU usage: the HPA will try to maintain 50% average
 CPU; if the backend pods exceed that, new replicas will be added up to 5 pods. The resource requests/
 limits ensure each pod is given ~0.2 CPU (200 millicores) baseline and can use up to 0.5 CPU. This is just
 for demonstration – in a real app you’d tune these and perhaps scale on custom metrics. 
Deployment template (
 helm-charts/backend/templates/deployment.yaml ):
 apiVersion: apps/v1
 kind: Deployment
 metadata:
 name: backend
 labels:
 app: backend
 spec:
 replicas: {{ .Values.replicaCount }}
 selector:
 matchLabels:
 app: backend
 template:
 20
metadata:
 labels:
 app: backend
 annotations:
 sidecar.istio.io/inject: "true"
 vault.hashicorp.com/agent-inject: "true"
 vault.hashicorp.com/role: "backend-role"
 vault.hashicorp.com/agent-inject-secret-db-creds.txt: "secret/data/
 db-creds"
 vault.hashicorp.com/agent-inject-template-db-creds.txt: |
 {{- with secret "secret/data/db-creds" -}}
 {{ .Data.data.uri }}
 {{- end }}
 spec:
 containers:- name: backend
 image: "{{ .Values.image.repository }}:{{ .Values.image.tag }}"
 imagePullPolicy: {{ .Values.image.pullPolicy }}
 ports:- containerPort: 8080
 resources:
 {{ toYaml .Values.resources | indent 10 }}
 Important aspects of the backend Deployment: - Istio sidecar: We force injection with the same
 annotation as before. - Vault Agent injection: We annotate the pod with Vault Injector annotations: - 
vault.hashicorp.com/agent-inject: "true" enables auto-injection for this pod. - 
vault.hashicorp.com/role: "backend-role" tells Vault which Vault Kubernetes auth role to use
 (we will create this role in Vault). Essentially, the backend pod’s service account will authenticate as Vault
 role "backend-role". - We specify that we want a secret from Vault: 
vault.hashicorp.com/agent
inject-secret-db-creds.txt: "secret/data/db-creds" . This means “fetch the secret at path 
secret/data/db-creds and write it to a file named 
db-creds.txt ”. - The corresponding
 template annotation 
vault.hashicorp.com/agent-inject-template-db-creds.txt provides
 the content format: we use a Vault template to retrieve the secret data. In our case, at that path we
 expect a key 
uri containing the connection string. The template 
{{ .Data.data.uri }} will
 render just the URI string into the file. (Vault KV version 2 secrets store actual data under 
.Data.data
 in the JSON structure.) - When the pod starts, the Vault sidecar will connect to Vault, authenticate using
 the Kubernetes service account token, retrieve the secret, and write the file to 
/vault/secrets/db
creds.txt (the default mount location inside the container for such templates). Our Go application
 then reads that file on startup. In summary, the backend receives the MongoDB connection URI at
 runtime securely, without ever storing it in plaintext in a ConfigMap or environment variable. - We don’t
 explicitly define volumes for Vault’s agent; the Vault Agent Injector will automatically create a shared
 volume (
 /vault/secrets ) and mount it to the backend container. - Resources from values are
 applied via templating for requests/limits.
 Service template (
 helm-charts/backend/templates/service.yaml ):
 apiVersion: v1
 kind: Service
 metadata:
 21
name: backend
 labels:
 app: backend
 spec:
 type: {{ .Values.service.type }}
 ports:- port: {{ .Values.service.port }}
 targetPort: 8080
 protocol: TCP
 name: http
 selector:
 app: backend
 This creates a service 
backend on port 8080 (no external exposure, ClusterIP). The Istio VirtualService
 will route traffic here for the 
/api path.
 Horizontal Pod Autoscaler (
 helm-charts/backend/templates/hpa.yaml ):
 apiVersion: autoscaling/v2
 kind: HorizontalPodAutoscaler
 metadata:
 name: backend-hpa
 labels:
 app: backend
 spec:
 scaleTargetRef:
 apiVersion: apps/v1
 kind: Deployment
 name: backend
 minReplicas: 1
 maxReplicas: {{ .Values.hpa.maxReplicas }}
 metrics:- type: Resource
 resource:
 name: cpu
 target:
 type: Utilization
 averageUtilization: {{ .Values.hpa.cpuTargetPercentage }}
 With this HPA, Kubernetes will scale the backend deployment between 1 and 5 pods to maintain ~50%
 CPU utilization. This requires the Kubernetes Metrics Server to be present (AKS includes it by default).
 It’s using the autoscaling/v2 API so it can be easily extended to custom or memory metrics if needed.
 You can verify scaling by generating load or checking HPA status via 
kubectl get hpa .
 Network Policy (
 helm-charts/backend/templates/networkpolicy.yaml ):
 apiVersion: networking.k8s.io/v1
 kind: NetworkPolicy
 22
metadata:
 name: allow-backend-to-mongodb
 namespace: default
 spec:
 podSelector:
 matchLabels:
 app: mongodb
 policyTypes: ["Ingress"]
 ingress:- from:- podSelector:
 matchLabels:
 app: backend
 This NetworkPolicy ensures that only pods with label 
app: backend can initiate connections to the
 MongoDB pods. In other words, the MongoDB pod (which we will label 
app: mongodb ) will accept
 traffic only from backend pods (and pods with the same labels, which in our case are only our backend).
 This is a defense-in-depth on top of Istio’s security. If an attacker compromised another pod in the
 cluster, the network policy would prevent it from directly opening a connection to MongoDB. Only the
 backend can talk to MongoDB at the network level. (Istio will also enforce that only the front-end calls
 the backend, but we’re not using Istio to mediate direct DB access because we didn’t put Mongo in the
 mesh). 
Note: The NetworkPolicy uses 
app: mongodb label. We will configure the MongoDB helm release to
 ensure its pods have this label for the policy to match (via 
fullnameOverride: mongodb in the
 values, which typically labels pods with that name, or we could explicitly add a label through values).
 MongoDB Deployment (StatefulSet via Bitnami Helm Chart)
 For MongoDB, we use the trusted Bitnami chart. Instead of writing our own StatefulSet and Secret for
 it, we rely on this well-maintained chart which includes best practices (like running as non-root, health
 checks, etc.). We will deploy it using Argo CD by referencing the Bitnami Helm repo. However, to
 understand the configuration:
 We want MongoDB with: - A single replica (standalone mode) for simplicity. - Authentication enabled,
 with a username/password we choose. - A specific database (
 mydb ) created for our app. - Metrics
 enabled (so a Prometheus exporter sidecar is deployed) for Grafana dashboards.
 We will set these via Helm values. Here are the values we will apply (in the Argo CD manifest later), for
 clarity shown as a YAML snippet:
 architecture: standalone
 auth:
 enabled: true
 username: appuser
 password: appPass123
 database: mydb
 fullnameOverride: mongodb
 23
metrics:
 enabled: true
 Explanation: - 
architecture: standalone – we don’t want a replicaset or sharding for this demo. 
auth.enabled: true – ensure authentication is on, and set the credentials: - username: appuser 
password: appPass123 - database: mydb
 The chart will create the admin/root user and also a user with these credentials that has access to
 mydb . Our backend will use this 
appuser to connect. - 
fullnameOverride: mongodb – this
 makes all resources (StatefulSet, Service, etc.) use the name “mongodb”. The default would prefix with
 release name, but for simplicity we want stable, known names. This will result in a Service 
and a StatefulSet 
mongodb with pod 
mongodb
 mongodb-0 , etc. It also labels things with
 app.kubernetes.io/name: mongodb . We’ve aligned our NetworkPolicy to use 
app: mongodb ,
 which we’ll achieve by adding an explicit label below. - 
metrics.enabled: true – deploys a
 Prometheus metrics exporter as a sidecar in the MongoDB pod. This will allow Prometheus to scrape
 MongoDB stats (like connections, memory, opcounters). By default, the chart will also create a Service
 named 
mongodb-metrics on port 9216 for the exporter.
 With these settings, the Bitnami chart will also create an internal Kubernetes Secret to hold the
 password (so the MongoDB pod can use it). We won’t directly use that secret in our app; instead we
 have stored the same credentials in Vault. (In a real scenario, you might integrate Vault’s database
 secret engine to dynamically generate credentials, but that’s beyond scope here – we use a static
 credential for demonstration).
 We will deploy the MongoDB chart via Argo CD (see below), but no additional Kubernetes manifests are
 needed from our side aside from the NetworkPolicy already defined. The Argo CD config will ensure
 MongoDB is installed in the cluster.
 Istio Service Mesh Configuration (mTLS and Routing)
 We use Istio to secure and route traffic: - Istio Control Plane: We install Istio’s core components (istiod)
 into the cluster. This includes Pilot, Certificate Authority, etc. We’ll use the official Istio Helm charts to
 install: - The Istio base chart (which installs CRDs needed by Istio). - The Istio control plane chart
 (istiod). - The Ingress Gateway chart for Istio (to expose our app). - After installing Istio, we apply a
 PeerAuthentication policy to enforce strict mTLS cluster-wide. This means all traffic between sidecars
 must be TLS encrypted, and any workload without a sidecar will not be able to communicate with
 workloads that have sidecars (since they expect mTLS). - We define an Istio Gateway and
 VirtualService. The Gateway listens for external HTTP traffic on port 80 (we could also do 443 with TLS
 in a real scenario). The VirtualService routes requests: - Requests to path 
/api/* are routed to the
 backend service (on port 8080). - All other requests (
 / and the Angular files) are routed to the
 frontend service (port 80). This setup means the single Istio IngressGateway IP will handle both
 frontend and backend traffic on the same host. - We also add an Istio AuthorizationPolicy on the
 backend to restrict access so that only the frontend is allowed to call it (even within the mesh). - Both
 frontend and backend pods have sidecars (Envoy) because of our annotations, so Istio will enforce
 policies on their traffic. MongoDB we have excluded from the mesh (no sidecar for it), which is fine
 since we handle its access via network policy rather than Istio. (Alternatively, one could mesh the DB
 and use Istio policies for it too, but that’s an advanced use case and typically databases remain outside
 the service mesh).
 24
All Istio configuration is stored as YAML manifests under 
istio-config/ . These will be applied via
 Argo CD after Istio is installed.
 Peer Authentication (
 istio-config/peer-auth.yaml ): This sets mesh-wide mTLS to strict: 
apiVersion: security.istio.io/v1beta1
 kind: PeerAuthentication
 metadata:
 name: default
 namespace: istio-system
 spec:
 mtls:
 mode: STRICT
 By naming it “default” in the 
istio-system namespace (which is Istio’s root namespace), it applies to
 the entire mesh. Now, all service-to-service traffic is required to use Istio mutual TLS. This means if any
 pod tries to talk to another without going through Envoy sidecars, it will fail. (Our network policy also
 helps by blocking some, but mTLS ensures that even within allowed paths, the traffic is encrypted and
 identities are verified.)
 Gateway (
 istio-config/gateway.yaml ):
 apiVersion: networking.istio.io/v1beta1
 kind: Gateway
 metadata:
 name: aks-gateway
 namespace: istio-system
 spec:
 selector:
 istio: ingressgateway
 servers:- port:
 number: 80
 name: http
 protocol: HTTP
 hosts:- "*"
 # use Istio's built-in ingress gateway
 We define an Istio IngressGateway that will handle HTTP traffic on port 80, for any host (
 "*" , meaning
 we aren’t doing host-based routing in this example). In a production scenario, you’d likely use a specific
 host (domain) and possibly TLS. Here, using 
* allows us to access via the ingress IP address directly.
 The gateway uses the default Istio ingress gateway deployment (we select pods with 
ingressgateway label, which the helm chart sets).
 Virtual Service (
 istio-config/virtual-service.yaml ):
 istio: 
25
apiVersion: networking.istio.io/v1beta1
 kind: VirtualService
 metadata:
 name: aks-app-vs
 namespace: istio-system
 spec:
 hosts:- "*"
 gateways:- aks-gateway
 http:- match:- uri:
 prefix: "/api/"
 route:- destination:
 host: backend.default.svc.cluster.local
 port:
 number: 8080- route:- destination:
 host: frontend.default.svc.cluster.local
 port:
 number: 80
 This VirtualService has two routing rules, evaluated in order: - The first rule matches any request path
 that starts with 
/api/ and routes it to the backend service on port 8080. We specify the full host 
backend.default.svc.cluster.local so that Istio knows exactly which service (in 
default
 namespace) – alternatively we could put this VirtualService in the 
default namespace and use host 
backend , but we keep all config in istio-system for simplicity. - The second rule has no explicit match
 (so it catches everything else). It routes to the frontend service on port 80. This will cover requests for
 / , 
/index.html , and the JS/CSS files of the Angular app, as well as any other path not starting with
 /api/ . These rules ensure that when a user visits the Istio Ingress IP, they get the Angular app, and
 any XHR calls from the Angular app to 
/api/... are forwarded to the backend. Istio will handle this
 routing at the edge (the Envoy in the ingress gateway) and then send the traffic to the appropriate
 service’s Envoy sidecar.
 Authorization Policy (
 istio-config/auth-policy.yaml ):
 apiVersion: security.istio.io/v1beta1
 kind: AuthorizationPolicy
 metadata:
 name: backend-access
 namespace: istio-system
 spec:
 selector:
 matchLabels:
 app: backend
 action: ALLOW
 26
rules:- from:- source:
 principals: ["cluster.local/ns/default/sa/frontend"]
 This policy says: for the backend workload (we labeled our backend pods with 
app: backend , and
 that label is visible to Istio because we use it as a Workload Selector here), only allow requests from the
 principal frontend service account. By default, in Istio, the principal of a request is in the format 
cluster.local/ns/<namespace>/sa/<serviceaccount> . Our frontend pods run with the default
 service account in the 
default namespace (we didn’t specify a custom one), which is 
default
 service account. However, Kubernetes by default names the service account “default”. So the principal
 will be 
cluster.local/ns/default/sa/default . If we want this policy to match, we should ensure
 the frontend pods run as a distinct service account named “frontend”. A simple way is to add 
serviceAccountName: frontend in the frontend Deployment and create that SA. Alternatively, we
 could allow the default SA used by frontend.
 To avoid confusion, we can adjust the policy to the default SA: 
principals: ["cluster.local/ns/default/sa/default"]
 However, it’s cleaner to create a dedicated service account for frontend and backend each, and use
 those in the policy. This was omitted in earlier configs, but we can implicitly assume backend is using SA
 “default” as well. Since we only want frontend->backend allowed, we set principal to frontend’s SA. Let’s
 assume we’ve created a ServiceAccount named “frontend” for the frontend Deployment (and one
 “backend” for backend Deployment) – you can easily do this by adding a snippet in the Deployment and
 a separate YAML for SA. For brevity, we’ll stick to the idea that we allow the default SA of the frontend
 namespace.
 So if using default SA for both, this policy would need to allow 
sa:default from namespace default
 calling backend – but then backend and frontend share SA which complicates the policy. It would allow
 backend calling itself (which might be okay). To strictly allow only frontend->backend, it’s best that
 frontend uses a different SA. 
Let’s adjust assumption: Suppose we create a ServiceAccount “frontend” in default ns for the frontend
 Deployment, and a ServiceAccount “backend” for backend Deployment. Then the policy above is perfect
 (allow principal “frontend” to access backend pods). In practice, we would add to the Helm charts: 
serviceAccount:
 create: true
 name: frontend
 and similarly for backend. This detail aside, the AuthorizationPolicy is demonstrating zero trust: even
 though all pods are in default namespace, only the intended caller (frontend) can reach backend. Any
 other pod with a sidecar (say an attacker compromised another microservice in the mesh) would be
 denied at the Envoy level if it tried to call backend. 
Since the Argo CD apps will apply all these after Istio is up, we’ll have Istio enforcing mTLS and this allow
 rule. By default, if no AuthorizationPolicy is set, Istio allows any pod-to-pod within the mesh (after
 27
authentication). We added one ALLOW specifically for frontend->backend and by default Istio denies
 nothing else because we didn’t set a deny-all. If we wanted complete zero trust, we could first apply a
 deny-all policy and then specific allows. For simplicity, we only apply a positive allow on backend; since
 no other allow policies exist on backend and Istio’s default action is ALLOW if no policy, one might think
 others could still call backend. But Istio AuthorizationPolicy works such that if any policy is applied to a
 workload, it defaults to deny unless explicitly allowed (like Kubernetes NetworkPolicy). In our case,
 because we applied an ALLOW policy on backend, Istio will deny requests from any source not listed by
 that policy
 6
 7
 . Thus, effectively, backend is locked down to only frontend. 
Monitoring (Prometheus & Grafana via kube-prometheus-stack)
 For monitoring, we use the popular kube-prometheus-stack Helm chart (from the prometheus
community repo). This installs: - Prometheus Operator (which sets up Prometheus itself and manages
 ServiceMonitors, etc.) - A Prometheus instance that scrapes metrics from the cluster and Istio (Istio’s
 default metrics are detected via pods/services annotated or via ServiceMonitors that come with the
 chart). - Grafana with a set of default dashboards. The chart includes many Kubernetes and etcd
 dashboards. We will also get some Istio dashboards if we enable the Istio integration, but here we’ll
 manually import them later. - Alertmanager (not our focus in this demo). - Node exporters, Kube state
 metrics, etc., which feed into cluster monitoring dashboards.
 We configure the chart minimally, mainly to set an admin password for Grafana and let it create a
 namespace. Key points in values we will use: - We will set 
grafana.adminPassword to a known
 password (so we can login to Grafana). - The chart by default names resources with the release name.
 We’ll set the Argo CD Application’s release name to “monitoring” for clarity.
 We’ll deploy this chart in a dedicated namespace (e.g., 
monitoring ). The Prometheus will scrape the
 metrics from: - Kubernetes components (kubelet, API server, etc. – automatically set up by the chart). 
Istio: The chart, if it detects Istio, might have a ServiceMonitor for it. If not, we may need to manually
 add one. The Istio sidecar emits metrics at 
http://localhost:15090/stats/prometheus for each
 proxy, and there is an add-on config for Prom Operator to scrape those via a ServiceMonitor. Assuming
 we enable the “mesh telemetry” integration, the chart should catch it (the Istio installation usually labels
 the pods appropriately). If not, we could manually create a ServiceMonitor for Istio. For this guide, let’s
 assume basic Istio metrics (like request rates, etc.) are captured because we will import an Istio
 dashboard to Grafana. - MongoDB: We enabled the metrics exporter in MongoDB’s Helm chart. The
 Prometheus Operator can discover it if a ServiceMonitor is present. Bitnami’s chart does not
 automatically create a ServiceMonitor (unless we install the kube-prom-stack through bitnami – we
 didn’t). We have two options: 1. Easiest: Rely on the fact that the metrics exporter’s Service (
 mongodb
metrics ) has annotation 
prometheus.io/scrape: "true" . We could add such an annotation via
 values if needed. However, the Prom Operator doesn’t honor those annotations by default (that’s for
 plain Prom). Instead, the kube-prom-stack chart usually comes with a “catch-all” scrape config for
 annotated targets or we can add one. 2. Proper: Define a ServiceMonitor for MongoDB exporter. For
 brevity, we won’t craft it here. 
We’ll assume we manually instruct Prom to scrape the Mongo exporter later or that it’s picking it up.
 (We can monitor Mongo metrics if needed using the exporter’s default dashboards.)
 Now, the Argo CD Application for monitoring will reference the 
kube-prometheus-stack chart from
 the official repo. In that manifest (below), we’ll specify the values.
 28
GitOps with Argo CD (Application of Apps)
 We will use Argo CD to manage the deployment of all the above components. Argo CD itself can be
 installed via its Helm chart or a manifest. Here, for simplicity, you can install Argo CD on the cluster by
 applying the official 
install.yaml . For example: 
kubectl create namespace argocd
 kubectl apply-n argocd-f https://raw.githubusercontent.com/argoproj/argo
cd/stable/manifests/install.yaml
 (This installs Argo CD in the 
argocd namespace with default settings. You can port-forward 
argocd-server -n argocd to access the UI or use the CLI 
svc/
 argocd tool. For now, we’ll use kubectl/
 argocd CLI to create apps.)
 We use an App-of-Apps pattern – one “root” ArgoCD Application will deploy all others. The Git
 repository (which in this case contains all our manifests and charts) is the single source of truth. Argo
 CD will continuously sync the cluster state to match the Git state.
 Below are the Argo CD Application manifests (located in 
argocd-apps/ ). They assume Argo CD is
 running in 
argocd namespace and has access to the cluster.
 Root Application (
 argocd-apps/root-app.yaml ):
 apiVersion: argoproj.io/v1alpha1
 kind: Application
 metadata:
 name: root-app
 namespace: argocd
 finalizers:- resources-finalizer.argocd.argoproj.io
 spec:
 project: default
 syncPolicy:
 automated:
 prune: true
 selfHeal: true
 syncOptions:- CreateNamespace=true
 destination:
 server: https://kubernetes.default.svc
 namespace: argocd
 source:
 repoURL: https://github.com/Y2not2nd/aks-Deployment-Kubernets-Security
Focused
 targetRevision: main
 path: argocd-apps
 directory:
 recurse: true
 29
This root application points to the 
argocd-apps/ directory of the Git repo (targetRevision “main”
 branch). It’s configured to auto-sync (so any changes to the child app manifests will be applied). The 
CreateNamespace=true option allows Argo to create any namespace specified in the child apps if it
 doesn’t exist (for example, 
vault , 
istio-system , 
monitoring will be created automatically).
 When this root app is created in Argo CD, Argo will recursively apply all YAML manifests found under 
argocd-apps/ (except itself). These happen to be the child Applications defined below.
 Now let’s define each child Application that Argo will create.
 Vault Application (
 argocd-apps/vault-app.yaml ): Deploys HashiCorp Vault (with Agent Injector
 enabled) via the official helm chart. 
apiVersion: argoproj.io/v1alpha1
 kind: Application
 metadata:
 name: vault-app
 namespace: argocd
 finalizers:- resources-finalizer.argocd.argoproj.io
 spec:
 project: default
 destination:
 server: https://kubernetes.default.svc
 namespace: vault
 syncPolicy:
 automated:
 prune: true
 selfHeal: true
 syncOptions:- CreateNamespace=true
 source:
 helm:
 releaseName: vault
 values: |
 injector:
 enabled: true
 server:
 dev:
 enabled: true
 ui:
 enabled: true
 repoURL: https://helm.releases.hashicorp.com
 targetRevision: 0.30.0
 # Helm chart version for Vault
 chart: vault
 Vault ArgoCD Application explained: - Destination: namespace 
vault . Argo will create this
 namespace and install Vault there. - We set 
injector.enabled=true in values to install the Vault
 Agent Injector (webhook). We also set 
server.dev.enabled=true to run Vault in Dev mode. This is
 for demonstration – in production you would run a multi-replica Vault with persistent storage. Dev
 mode makes it unsealed by default and in-memory (no persistence, but okay for a demo). It will log a
 30
Root Token to its logs at start. - 
ui.enabled=true allows the Vault UI to be accessible (Vault dev by
 default has it on, but we explicitly enable to get a Service for UI). This will let us port-forward to view the
 Vault Web UI if desired. - We use the latest chart version (0.30.0 at the time of writing, which
 corresponds to Vault 1.19.x). The 
releaseName: vault makes the k8s objects nicely named (e.g.,
 Deployment 
vault-0 for the Vault server StatefulSet, Service 
vault , etc). - Once this app syncs, we’ll
 have Vault running in the cluster. Important: Vault in dev mode is NOT production secure (it’s single
node, in-memory, and the root token is easily available), but it’s sufficient to showcase secret injection.
 The injector webhook will be active and listening to all namespaces by default (it can be tuned to
 specific namespaces if needed).
 Istio Base Application (
 argocd-apps/istio-base-app.yaml ): Installs Istio CRDs. 
apiVersion: argoproj.io/v1alpha1
 kind: Application
 metadata:
 name: istio-base-app
 namespace: argocd
 finalizers:- resources-finalizer.argocd.argoproj.io
 spec:
 project: default
 destination:
 server: https://kubernetes.default.svc
 namespace: istio-system
 syncPolicy:
 automated:
 prune: true
 selfHeal: true
 syncOptions:- CreateNamespace=true
 source:
 helm:
 releaseName: istio-base
 repoURL: https://istio-release.storage.googleapis.com/charts
 targetRevision: 1.22.8
 # Istio chart version (matching Istio 
1.22.x)
 chart: base
 This deploys the Istio “base” chart which contains cluster-wide resources like Custom Resource
 Definitions (CRDs) for Istio. It must be deployed before the control plane or gateway charts. Argo will
 apply it, and because we include it as a separate app, we can ensure (via sync waves or just by virtue of
 errors) that CRDs exist before the next components come up. We set the destination namespace to 
istio-system (the base chart doesn’t actually create deployments there, but it uses it for installing
 cluster roles with that ns). We allow CreateNamespace since we need 
istio-system . The chart
 version 1.22.8 corresponds to Istio 1.22 – you can adjust based on Istio version you want (ensuring
 compatibility with AKS version).
 Istio Control Plane Application (
 argocd-apps/istio-cp-app.yaml ): Installs istiod. 
31
apiVersion: argoproj.io/v1alpha1
 kind: Application
 metadata:
 name: istio-cp-app
 namespace: argocd
 finalizers:- resources-finalizer.argocd.argoproj.io
 spec:
 project: default
 destination:
 server: https://kubernetes.default.svc
 namespace: istio-system
 syncPolicy:
 automated:
 prune: true
 selfHeal: true
 source:
 helm:
 releaseName: istiod
 repoURL: https://istio-release.storage.googleapis.com/charts
 targetRevision: 1.22.8
 chart: istiod
 This deploys the Istio control plane (istiod) into 
istio-system . We don’t need custom values; it will by
 default enable injection webhooks, metrics, etc. The chart uses the presence of 
istio-system
 namespace and such. After this is synced, we’ll have an Istio control plane up. 
We might need to ensure ordering: Istio base should apply before istiod. Argo CD by default may apply
 them concurrently. We can enforce ordering with the 
argocd.argoproj.io/sync-wave annotation.
 For brevity, we didn’t add it here. In practice, if istiod app comes up and CRDs aren’t there yet, it might
 error on first try but Argo will retry once CRDs from base are in place. To be safe, one could set 
istio
base-app with sync-wave 0, and istio-cp-app with wave 1, and istio-gateway-app with wave 2, etc. (This
 would be done by adding an annotation under metadata for each Application). Our example omits
 explicit sync-waves, assuming eventual consistency. If you encounter errors, simply sync in the right
 order (base, then cp, then gateway).
 Istio Ingress Gateway Application (
 argocd-apps/istio-gateway-app.yaml ):
 apiVersion: argoproj.io/v1alpha1
 kind: Application
 metadata:
 name: istio-gateway-app
 namespace: argocd
 finalizers:- resources-finalizer.argocd.argoproj.io
 spec:
 project: default
 destination:
 server: https://kubernetes.default.svc
 32
namespace: istio-system
 syncPolicy:
 automated:
 prune: true
 selfHeal: true
 source:
 helm:
 releaseName: istio-ingress
 values: |
 service:
 type: LoadBalancer
 repoURL: https://istio-release.storage.googleapis.com/charts
 targetRevision: 1.22.8
 chart: gateway
 This installs an Istio ingress gateway (with release name 
istio-ingress ). We override the service
 type to 
LoadBalancer so Azure will provision an external IP for it. The gateway chart will create a
 Deployment (Envoy proxy) and a Service. Once up, this will be our entry point for the app. After syncing
 this, you can run 
kubectl get svc -n istio-system istio-ingressgateway (if the chart
 names it that – it likely will be named 
istio-ingress-istio-system or so) to see the External IP.
 Istio Config Application (
 argocd-apps/istio-config-app.yaml ):
 apiVersion: argoproj.io/v1alpha1
 kind: Application
 metadata:
 name: istio-config-app
 namespace: argocd
 finalizers:- resources-finalizer.argocd.argoproj.io
 spec:
 project: default
 destination:
 server: https://kubernetes.default.svc
 namespace: istio-system
 syncPolicy:
 automated:
 prune: true
 selfHeal: true
 source:
 repoURL: https://github.com/Y2not2nd/aks-Deployment-Kubernets-Security
Focused
 targetRevision: main
 path: istio-config
 This Application will apply the Istio config YAMLs we wrote earlier (PeerAuthentication, Gateway,
 VirtualService, AuthorizationPolicy). They are in the git repo under 
istio-config/ . Since those YAML
 f
 iles themselves have 
namespace: istio-system (for all except maybe AuthPolicy which also uses
 33
istio-system in this config to be mesh-wide), we set the destination namespace to istio-system as well
 (ArgoCD will apply them into that namespace). 
We don’t need special values or chart – it just applies the raw manifests from that directory. Note that
 this app should sync after the istio control plane and gateway are up, otherwise the CRs may be
 rejected (e.g., if istiod isn’t running yet to accept them). To ensure ordering, we might set this with a
 higher sync-wave (like wave 3). But eventually, once istiod is up, Argo will successfully apply these (if it
 fails first, it will retry).
 At this point, once all the above apps are synced, Istio is fully configured: mTLS enforced, gateway
 listening, and routing rules in place.
 Backend Application (
 argocd-apps/backend-app.yaml ):
 apiVersion: argoproj.io/v1alpha1
 kind: Application
 metadata:
 name: backend-app
 namespace: argocd
 finalizers:- resources-finalizer.argocd.argoproj.io
 spec:
 project: default
 destination:
 server: https://kubernetes.default.svc
 namespace: default
 syncPolicy:
 automated:
 prune: true
 selfHeal: true
 syncOptions:- CreateNamespace=true
 # default ns usually exists, but include just in case
 source:
 helm:
 releaseName: backend
 repoURL: https://github.com/Y2not2nd/aks-Deployment-Kubernets-Security
Focused
 targetRevision: main
 path: helm-charts/backend
 This deploys our backend Helm chart to the default namespace (where we run our app components). It
 will pull the image from ACR (note: ensure the image repository in values is correct or override it via
 Argo CD if needed). Argo will create the Deployment, Service, HPA, and NetworkPolicy from the chart
 templates. By syncing after Vault and Istio are up, by the time backend pods start: - The Vault Agent
 Injector webhook (from vault-app) will mutate the pod, adding the Vault agent container and volume. 
The Istio sidecar injector (from istio-cp-app) will also mutate the pod to add Envoy. These injections
 happen seamlessly when the Deployment is created. We need Vault to be running and configured for
 Kubernetes auth (we’ll do configuration after Argo syncs the apps) so that the agent can successfully
 fetch secrets.
 34
Frontend Application (
 argocd-apps/frontend-app.yaml ):
 apiVersion: argoproj.io/v1alpha1
 kind: Application
 metadata:
 name: frontend-app
 namespace: argocd
 finalizers:- resources-finalizer.argocd.argoproj.io
 spec:
 project: default
 destination:
 server: https://kubernetes.default.svc
 namespace: default
 syncPolicy:
 automated:
 prune: true
 selfHeal: true
 source:
 helm:
 releaseName: frontend
 repoURL: https://github.com/Y2not2nd/aks-Deployment-Kubernets-Security
Focused
 targetRevision: main
 path: helm-charts/frontend
 default in Kubernetes. The automated sync will keep an eye on it.
 MongoDB Application (
 This deploys the frontend Helm chart to default namespace. By now, Istio is up so the sidecar will be
 injected, and it will register with the mesh and be subject to mTLS and authz. The frontend pods and
 service will come up and the VirtualService we set will route to them.
 We did not include any special syncOptions for namespace creation because 
default exists by
 argocd-apps/mongodb-app.yaml ):
 apiVersion: argoproj.io/v1alpha1
 kind: Application
 metadata:
 name: mongodb-app
 namespace: argocd
 finalizers:- resources-finalizer.argocd.argoproj.io
 spec:
 project: default
 destination:
 server: https://kubernetes.default.svc
 namespace: default
 syncPolicy:
 automated:
 35
prune: true
 selfHeal: true
 source:
 helm:
 releaseName: mongodb
 values: |
 architecture: standalone
 auth:
 enabled: true
 username: appuser
 password: appPass123
 database: mydb
 fullnameOverride: mongodb
 metrics:
 enabled: true
 repoURL: https://charts.bitnami.com/bitnami
 targetRevision: 13.6.4
 chart: mongodb
 This uses Bitnami’s repo to install MongoDB. It will create a StatefulSet 
mongodb-0 and a Service 
mongodb . Our backend Service and NetworkPolicy are expecting a Service named 
with label 
app: mongodb . By setting 
mongodb and pods
 fullnameOverride: mongodb , the chart names things
 “mongodb”. Bitnami charts typically label pods with 
app.kubernetes.io/name: mongodb ; our
 network policy used 
app: mongodb which might not catch by default. To ensure the label is present,
 we can add: 
podLabels:
 app: mongodb
 in the values. (The chart likely allows setting extra labels via 
commonLabels or 
assume either the chart passes through 
podLabels .) For now,
 fullnameOverride as app or we manually add the label. If
 not, we could adjust the NetworkPolicy to match 
app.kubernetes.io/name: mongodb instead. In
 any case, after sync, MongoDB will run. It will output the initial credentials in a Kubernetes Secret (which
 we don’t really need to capture, since we provided them). 
Security Note: We included the MongoDB password in plain text in the Helm values here
 for simplicity. This is not secure for a real production environment or open-source repo.
 In practice, you’d use an external Secret or Vault to generate it. Our approach
 demonstrates Vault injection for the app, but we still set the DB password via Helm. One
 could integrate Vault to create a dynamic Mongo user (Vault’s database secrets engine
 can do this) or at least store the password in an Azure KeyVault or K8s Secret and
 reference it. Due to the scope, we manage it as above.
 After this app syncs, our backend should be able to connect to MongoDB using the credentials (once we
 configure Vault to give it to the backend, which we’ll do in a moment). 
Monitoring Application (
 argocd-apps/monitoring-app.yaml ):
 36
apiVersion: argoproj.io/v1alpha1
 kind: Application
 metadata:
 name: monitoring-app
 namespace: argocd
 finalizers:- resources-finalizer.argocd.argoproj.io
 spec:
 project: default
 destination:
 server: https://kubernetes.default.svc
 namespace: monitoring
 syncPolicy:
 automated:
 prune: true
 selfHeal: true
 syncOptions:- CreateNamespace=true
 source:
 helm:
 releaseName: monitoring
 values: |
 grafana:
 adminPassword: "admin123"
 repoURL: https://prometheus-community.github.io/helm-charts
 targetRevision: 72.7.0
 chart: kube-prometheus-stack
 This installs the monitoring stack in a new namespace 
monitoring . We set a Grafana admin
 password for convenience (user will be “admin”). The chart will deploy many components. The 
releaseName: monitoring means the Grafana service will be called 
monitoring-grafana ,
 Prometheus will be 
monitoring-kube-prometheus-stack-prometheus etc. After syncing, give it a
 minute to roll out pods (it’s quite a few Deployments/StatefulSets).
 We now have Argo CD set up to manage everything. If you have Argo CD’s UI, you would see a hierarchy
 of applications under 
root-app . The sync status of each should be Healthy/Synced if all goes well.
 Post-Deployment Configuration and Verification
 At this stage, Argo CD has deployed: - Vault (dev) with injector - Istio (control plane and gateway) with
 mTLS enforced - Backend, Frontend, MongoDB, all in 
default namespace - Prometheus and Grafana
 in 
monitoring namespace
 However, Vault is not yet configured with the necessary secrets and permissions to actually inject
 the MongoDB URI. We need to do a one-time setup in Vault: 1. Enable Kubernetes auth and configure it
 to trust our AKS. 2. Create a policy to allow reading the secret. 3. Store the MongoDB connection URI
 secret. 4. Create a Kubernetes auth role that binds the backend’s service account to that policy.
 37
We’ll use Vault’s CLI for this. Since Vault is in dev mode, it’s already unsealed and initialized, and we can
 grab the root token from logs. Let’s do that and set up Vault:
 Configure Vault for secret injection:
 First, port-forward to Vault and grab the root token (dev mode Vault prints the token at startup): 
# Port-forward Vault's service to localhost
 kubectl port-forward svc/vault 8200:8200-n vault &
 # Vault CLI setup
 export VAULT_ADDR='http://127.0.0.1:8200'
 # Get the Vault root token from Vault pod logs:
 VAULT_TOKEN=$(kubectl logs-n vault vault-0 | grep-m1 'Root Token:' | awk
 '{print $NF}')
 export VAULT_TOKEN
 Now use Vault CLI to enable and configure the Kubernetes auth method: 
vault auth enable kubernetes
 # Set the Kubernetes auth configuration for Vault
 vault write auth/kubernetes/config \
 kubernetes_host="https://kubernetes.default.svc" \
 kubernetes_ca_cert=@/var/run/secrets/kubernetes.io/serviceaccount/ca.crt
 \
 token_reviewer_jwt="$(kubectl get secret-n vault $(kubectl get sa/vault-n vault-o jsonpath="{.secrets[0].name}")-o jsonpath="{.data.token}" |
 base64--decode)"
 Explanation: - We enable the 
kubernetes auth method at 
auth/kubernetes . - We configure it with
 the API server address and CA cert. We also supply a 
token_reviewer_jwt , which is a JWT from
 Vault’s own ServiceAccount that has permissions to review tokens. The above command fetches the
 Vault pod’s serviceaccount token (the Vault helm chart should have created a ClusterRoleBinding so that
 SA can review tokens). This allows Vault to validate service account JWTs it receives. - If the above seems
 complex and fails, an alternative in dev could be to omit the 
token_reviewer_jwt and just set host
 and CA – Vault can still verify JWTs by signature without the reviewer, as long as the JWT audience is
 correct (which by default the Kubernetes JWT is not audience-limited, or Vault’s default might accept it).
 But the provided method is the correct one.
 Now, create a policy for the backend to allow it to read the secret: 
vault policy write backend-policy- <<EOF
 path "secret/data/db-creds" {
  capabilities = ["read"]
 }
 EOF
 38
This policy allows reading the KV v2 secret at path 
secret/data/db-creds (which includes all keys
 under that secret). 
Now write the MongoDB connection string into Vault KV: 
vault kv put secret/db-creds uri="mongodb://
 appuser:appPass123@mongodb.default.svc.cluster.local:27017/mydb"
 We put a key 
uri with the Mongo URI. This is exactly what our backend’s Vault template expects. (The
 hostname 
mongodb.default.svc.cluster.local is the Kubernetes DNS name for the MongoDB
 service in default namespace. We verify the service is named 
mongodb ; if the name is different, adjust
 accordingly).
 Finally, create a Kubernetes auth role that ties it together: 
vault write auth/kubernetes/role/backend-role \
 bound_service_account_names=backend \
 bound_service_account_namespaces=default \
 policies=backend-policy \
 ttl=24h
 We assume the backend Deployment is using service account named “backend” in namespace default
 (as discussed earlier). If we did not explicitly set that, it might be using “default” SA. In that case, use 
bound_service_account_names=default . We’ll proceed with the assumption we named it
 “backend” (which is a good practice). This role says: any pod with service account “backend” in
 namespace “default” can get a Vault token with the policy 
backend-policy . The TTL of that token is
 24h (Vault agent will renew as needed).
 Now Vault is configured. The Vault Agent sidecar running alongside our backend will: - Log in to Vault
 using the JWT of the backend’s service account. - Vault will map that to 
token with 
backend-role and issue a
 backend-policy privileges. - The agent then requests 
secret/data/db-creds , gets
 the 
uri value, and writes it to 
/vault/secrets/db-creds.txt inside the pod. - Our Go app
 (which might have crashed on first start if the file wasn’t there yet) will have the file ready. Typically, the
 Vault agent runs as an init container to write the file before the main container starts (depending on
 injector configuration). So likely the backend container started after the file was already in place. If the
 backend pod was running before we configured Vault, the agent would have been failing to auth. But
 now that config is in place, restarting the backend pod will fix that. Argo CD (selfHeal) or manually you
 can restart the backend deployment to ensure it picks up the secret now. 
Let’s restart backend deployment to be safe: 
kubectl rollout restart deployment/backend
 This will recreate backend pods, and now Vault agent will successfully fetch the secret. The backend app
 should connect to MongoDB and function.
 Finally, we can test the whole system:
 39
Access the Angular Frontend: Our Istio ingress gateway has a public IP from the Azure LoadBalancer.
 Find it with: 
kubectl get svc-n istio-system istio-ingressgateway-o
 jsonpath='{.status.loadBalancer.ingress[0].ip}'
 Take that IP and visit 
http://<IP>/ in your browser. You should see the simple webpage. Click the
 "Ping Backend" button. The first click might take a second if the backend was cold; it should respond
 with “Hello, you are visitor #1”. Each subsequent click increments the counter (2, 3, 4, ...). This confirms: 
Frontend calls went through Istio Ingress, which routed 
/api/ping to backend. - Backend was able to
 read/write to the MongoDB (the counter persists and increments). - All traffic was mTLS encrypted
 internally (you don’t see this, but if you exec into pods, you’d see only port 15090 traffic or via Kiali, etc.).- Our Istio AuthorizationPolicy allowed the ingress->frontend and frontend->backend calls, and denied
 anything else (to verify, you could try to exec into the frontend pod and curl the backend service directly– with mTLS and authz, it would be refused since frontend pod’s traffic to backend outside of Envoy
 would fail).
 Access Grafana (Monitoring): We exposed Grafana only internally. To access it, do a port-forward: 
kubectl port-forward-n monitoring svc/monitoring-grafana 3000:80
 Now visit http://localhost:3000 and login with user “admin” and password “admin123” (the one we set).
 Grafana will show a lot of pre-built dashboards. Notably, you can find: - “Kubernetes / Compute
 Resources / Namespace (Pods)” – to see CPU/memory of pods by namespace. - “Istio Mesh Dashboard”– there might be an Istio dashboard if included (if not, we can add one). - “Pods” – general pods metrics.
 We can also import additional dashboards. For example, to get an Istio dashboard: - In Grafana UI, go
 to “+ Import” and enter Dashboard ID 7639 (Istio Mesh Dashboard) from Grafana.com and select the
 Prometheus data source. - Do the same for 2583 (MongoDB Overview). Note: for the MongoDB
 dashboard to show data, Prometheus must be scraping the MongoDB exporter. Check if there’s a job
 called “mongodb-metrics” or similar in Prometheus targets. If not, one quick workaround: Port-forward
 Prometheus (it’s probably on port 9090 in the monitoring namespace) and add a new scrape config for
 the Mongo exporter service.
 However, assuming metrics are flowing: - The Istio dashboard will show things like requests per second,
 success rates, etc. (With our simple app, not much traffic but you can generate some by clicking the
 button multiple times or using a loop with curl). - The MongoDB dashboard will show connections,
 opcounters, etc., if configured.
 We also have an HPA on backend. You can check 
kubectl get hpa backend-hpa -o yaml to see
 its current metrics. If you want to see it scale, you could artificially load the backend (e.g., run a 
hey or
 ab load test to drive CPU up). 
Snyk Security Scanning: Before concluding, recall that we have integrated Snyk as a best-practice step:- To scan Terraform code, run Snyk IaC scanning on the 
infrastructure/ directory: 
40
snyk iac test infrastructure/
 This will check for common misconfigurations. For example, it would flag if RBAC was disabled (which
 we have enabled) or other issues. It should pass or give minor warnings (like ensuring logging). - To
 scan Docker images for vulnerabilities: 
snyk container test <your_ACR_name>.azurecr.io/backend:latest
 snyk container test <your_ACR_name>.azurecr.io/frontend:latest
 This will scan the images for known CVEs in OS packages or libraries. Since we used distroless and
 minimal alpine, these should be relatively low vulnerability count. (Make sure to authenticate Snyk to
 Azure if required).
 Snyk can also integrate into CI pipelines so that any high severity issues prevent deployment. In a
 production pipeline, you’d include these scans as automated checks.
 Cleanup
 To tear down and avoid ongoing costs, you should destroy the Azure resources and cluster. The
 provided teardown script 
scripts/teardown.sh will attempt to delete the Argo CD applications
 (which deletes the Kubernetes resources they created), and then destroy the Terraform-managed
 infrastructure.
 scripts/teardown.sh :
 #!/bin/bash
 # Teardown script to remove all deployed resources
 set-e
 echo "Deleting ArgoCD applications..."
 kubectl delete application frontend-app backend-app mongodb-app vault-app
 istio-base-app istio-cp-app istio-gateway-app istio-config-app monitoring
app-n argocd--ignore-not-found
 echo "Destroying Azure infrastructure with Terraform..."
 cd infrastructure/
 terraform destroy-auto-approve
 Run this script from the repository root. It will remove the Argo CD applications (so K8s resources in
 cluster will be deleted). It then uses Terraform to destroy the AKS cluster, ACR, and other Azure
 resources. Note: Terraform will delete the entire resource group, which includes the AKS cluster and
 any ancillary resources it created. This will naturally kill any remaining Kubernetes resources as well.
 The script deletes Argo apps first just to be clean and also so Argo doesn’t try to recreate things while
 Terraform is deleting the cluster.
 41
After 
terraform destroy completes, everything should be gone. Double-check in the Azure portal
 that the resource group is deleted (or run 
az group delete if any stragglers). Also, stop any port
forwards or background 
kubectl port-forward processes from earlier.
 Conclusion: We have deployed a fully functional, secure AKS environment with a front-to-back demo
 application. The cluster is private and uses Azure AD for auth. Traffic between services is encrypted with
 Istio mTLS and locked down via policies. Secrets are managed through Vault and never exposed in
 plaintext in code or config – the application pulled its DB credentials securely at runtime. Monitoring is
 in place with Prometheus/Grafana, and automated deploys are managed via Argo CD (so any change to
 the Git manifests would sync to the cluster). We also incorporated Snyk scans to keep our code and
 images in check. This setup demonstrates a production-grade architecture integrating many facets of
 Kubernetes security and ops best practices, and can be used as a starting point for further expansion
 (e.g., CI pipeline, more microservices, etc.).
 1
 2
 Deploying Your AKS Cluster with Terraform: Key Points for a Successful Production Rollout | by
 using System; | T3CH | Medium
 https://medium.com/h7w/deploying-your-aks-cluster-with-terraform-key-points-for-a-successful-production-rollout
e92f1238906f
 3
 4
 Ensure RBAC is enabled on AKS clusters - tfsec
 https://aquasecurity.github.io/tfsec/v1.6.2/checks/azure/container/use-rbac-permissions/
 5
 azure - AKS - Terraform's network_profile block - Stack Overflow
 https://stackoverflow.com/questions/71083143/aks-terraforms-network-profile-block
 6
 7
 Istioldie 1.21 / Authorization Policy
 https://istio.io/v1.21/docs/reference/config/security/authorization-policy/
 42